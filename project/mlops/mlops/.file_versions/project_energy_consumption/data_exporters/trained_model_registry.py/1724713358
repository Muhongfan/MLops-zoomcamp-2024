import mlflow
import os
import pickle
import boto3
import pandas as pd
from mlflow.tracking import MlflowClient
from mlflow.models import infer_signature
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_extraction import DictVectorizer
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline


if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter

TRACKING_URI = os.environ.get("TRACKING_URI", "http://localhost:5000")
EXPERIMENT_NAME = os.environ.get("EXPERIMENT_NAME", "energy-prediction-pipeline")
mlflow.set_tracking_uri(TRACKING_URI)
mlflow.set_experiment(EXPERIMENT_NAME)
mlflow.sklearn.autolog()

@data_exporter
def export_data(model_info,data_info, *args, **kwargs) -> str:
    model, hyperparameters, verbose_eval, _ = model_info
    X, _, _, y, _, _, dv = data_info
    df_train, df_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    train = pd.concat([df_train, y_train], axis=1)
    test = pd.concat([df_test, y_test], axis=1)

    # Convert Timestamps to strings
    df_train['created_date'] = df_train['created_date'].astype(str)
    df_test['created_date'] = df_test['created_date'].astype(str)
    # Convert DataFrame rows to dictionaries
    df_train_dicts = df_train.to_dict(orient='records')
    df_test_dicts = df_test.to_dict(orient='records')


    os.makedirs('models', exist_ok=True)
    artifact_uri = ''
    run_id = ''

    with mlflow.start_run() as run:
        # basic info
        mlflow.set_tag("ML Engineer", "Amber Mu")
        mlflow.log_param("model_type", "RandomForest")
        mlflow.log_params(hyperparameters)

        pipeline = make_pipeline(
            DictVectorizer(),
            RandomForestRegressor(**params, n_jobs=-1)
        )
        pipeline.fit(df_train_dicts, y_train)
        y_pred = pipeline.predict(df_test_dicts)

        # Calculate and log metrics
        rmse = mean_squared_error(y_pred, y_test, squared=False)
        r2 = r2_score(y_test, y_pred)

        mlflow.log_metric('rmse', rmse)
        mlflow.log_metric("r2", r2)
        # Log the training and test datasets
        mlflow.log_artifact(train, artifact_path="datasets/training")
        mlflow.log_artifact(test, artifact_path="datasets/testing")
        # Log the model
        mlflow.sklearn.log_model(pipeline, artifact_path="models")

        signature = infer_signature(df_test.to_pandas(), y_pred)
        # Log Model
        model_info = mlflow.sklearn.log_model(
            sk_model=model, 
            artifact_path="models",
            registered_model_name="energy-consumption-prediction-pipeline",
            signature=signature
        )
        
        artifact_uri = mlflow.get_artifact_uri()
        run_id = run.info.run_id
        print(f"default artifact URI: '{artifact_uri}'")
        print(f"RunID: '{run.info.run_id}'")
        # Predict
        y_pred = model.predict(X_test)
        rmse = root_mean_squared_error(Y_test, y_pred)
        mlflow.log_metric("rmse", rmse)
        # Model Score
        model_score = model.score(X_test, Y_test)
        mlflow.log_metric("score", model_score)
    
    # Register Model
    # client = MlflowClient(TRACKING_URI)
    logged_model = f"runs:/{run_id}/models"
    mlflow.register_model(logged_model, f"{EXPERIMENT_NAME}")

    # Update AWS Lambda - Uncomment this if you want automtize this
    # client = boto3.client('lambda')
    # client.update_function_configuration(
    #     FunctionName='anemia-mlops-anemia-service',
    #     Environment={
    #         'Variables': {
    #             'RUN_ID': run_id
    #         }
    #     }
    # )
    return logged_model, artifact_uri, run_id